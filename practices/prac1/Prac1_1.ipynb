{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4NMHBq16Y_g"
      },
      "source": [
        "# RL basics\n",
        "\n",
        "Термины и понятия:\n",
        "\n",
        "- агент/среда\n",
        "- наблюдение $o$ / состояние $s$\n",
        "- действие $a$, стратегия $\\pi: \\pi(s) \\rightarrow a$ функция перехода $T: T(s, a) \\rightarrow s'$\n",
        "- вознаграждение $r$, ф-я вознаграждений $R: R(s, a) \\rightarrow r$\n",
        "- цикл взаимодействия, траектория $\\tau: (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)$, эпизод\n",
        "- отдача $G$, подсчет отдачи, средняя[/ожидаемая] отдача $\\mathbb{E}[G]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9JPaLF5v6esZ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGAoJeNF6hJO"
      },
      "source": [
        "## Agent, environment\n",
        "\n",
        "<img src=https://gymnasium.farama.org/_images/lunar_lander.gif caption=\"lunar lander\" width=\"150\" height=\"50\"><img src=https://gymnasium.farama.org/_images/mountain_car.gif caption=\"mountain car\" width=\"150\" height=\"50\">\n",
        "<img src=https://gymnasium.farama.org/_images/cliff_walking.gif caption=\"cliff walking\" width=\"300\" height=\"50\">\n",
        "<img src=https://ale.farama.org/_images/montezuma_revenge.gif caption=\"montezuma revenge\" width=\"150\" height=\"100\">\n",
        "<img src=https://github.com/danijar/crafter/raw/main/media/video.gif caption=\"crafter\" width=\"150\" height=\"100\">\n",
        "<img src=https://camo.githubusercontent.com/6df2ca438d8fe8aa7a132b859315147818c54af608f8609320c3c20e938acf48/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f344e78376759694d394e44724d724d616f372f67697068792e676966 caption=\"malmo minecraft\" width=\"150\" height=\"100\">\n",
        "<img src=https://images.ctfassets.net/kftzwdyauwt9/e0c0947f-1a44-4528-4a41450a9f0a/2d0e85871d58d02dbe01b2469d693d4a/table-03.gif caption=\"roboschool\" width=\"150\" height=\"100\">\n",
        "<img src=https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png caption=\"Марковский процесс принятия решений\" width=\"150\" height=\"100\">\n",
        "<img src=https://minigrid.farama.org/_images/DoorKeyEnv.gif caption=\"minigrid\" width=\"120\" height=\"120\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcyLKga76mA_"
      },
      "source": [
        "## Observation, state\n",
        "\n",
        "TODO:\n",
        "- добавить примеры наблюдений/состояний (числа, векторы, картинки)\n",
        "- интуитивное объяснение различия, положить пока, что наблюдение = состояние\n",
        "- пространство состояний\n",
        "\n",
        "\n",
        "В каждый момент времени среда имеет некоторое внутреннее состояние. Здесь слово \"состояние\" я употребил скорее в интуитивном понимании, чтобы обозначить, что среда изменчива (иначе какой смысл с ней взаимодействовать, если ничего не меняется). В обучении с подкреплением под термином состояние $s$ (или $s_t$, где $t$ — текущее время) подразумевают либо абстрактно информацию о \"состоянии\" среды, либо ее явное представление в виде данных, достаточные для полного описания \"состояния\". *NB: Здесь можно провести аналогию с компьютерными играми — файл сохранения игры как раз содержит информацию о \"состоянии\" мира игры, чтобы в будущем можно было продолжить с текущей точки, так что данные этого файла в целом можно с некоторой натяжкой считать состоянием (с натяжкой, потому что редко когда в сложных играх файлы сохранения содержат прямо вот всю информацию, так что после перезагрузки вы получите не совсем точную копию). При этом обычно подразумевается, что состояние не содержит в себе ничего лишнего, то есть это **минимальный** набор информации.*\n",
        "\n",
        "Наблюдением $o$ называют то, что агент \"видит\" о текущем состоянии среды. Это не обязательно зрение, а вообще вся доступная ему информация (условно, со всех его органов чувств).\n",
        "\n",
        "В общем случае наблюдение: кортеж/словарь многомерных векторов чисел."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ypHv9w6i6pcX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4,)\n",
            "(2,)\n"
          ]
        }
      ],
      "source": [
        "print(gym.make(\"CartPole-v1\").reset()[0].shape)\n",
        "print(gym.make(\"MountainCar-v0\").reset()[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GabuCLcJ67lb"
      },
      "source": [
        "## Action, policy, transition function\n",
        "\n",
        "Рассмотрим следующие MDP:\n",
        "\n",
        "- A: <img src=https://i.ibb.co/mrCMVZLQ/mdp-a.png caption=\"A\" width=\"400\" height=\"100\">\n",
        "- B: <img src=https://i.ibb.co/GQ2tVtjC/mdp-b.png caption=\"B\" width=\"400\" height=\"100\">\n",
        "\n",
        "Links to all:\n",
        "[A](https://i.ibb.co/mrCMVZLQ/mdp-a.png)\n",
        "[B](https://i.ibb.co/GQ2tVtjC/mdp-b.png)\n",
        "[C](https://i.ibb.co/Jj9LYHjP/mdp-c.png)\n",
        "[D](https://i.ibb.co/Y47Mr83b/mdp-d.png)\n",
        "[E](https://i.ibb.co/Kjt1Xhmf/mdp-e.png)\n",
        "\n",
        "Давайте явно запишем пространства состояний $S$ и действий $A$, а также функцию перехода $T$ среды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4XpqNc_o6_CS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "states_A={0, 1, 2} | actions_A={0}\n",
            "Transition function T_A={(0, 0): 1, (1, 0): 2, (2, 0): 2}\n"
          ]
        }
      ],
      "source": [
        "states_A   = set(range(3))\n",
        "actions_A = set(range(1))\n",
        "print(f'{states_A=} | {actions_A=}')\n",
        "T_A = {\n",
        "    (0, 0): 1,\n",
        "    (1, 0): 2,\n",
        "    (2, 0): 2\n",
        "}\n",
        "print(f'Transition function {T_A=}')\n",
        "A_mdp = states_A, actions_A, T_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "states_B={0, 1, 2, 3} | actions_B={0, 1, 2}\n",
            "Transition function T_B={(0, 0): 1, (0, 1): 2, (0, 2): 3, (1, 0): 1, (1, 1): 1, (1, 2): 1, (2, 0): 2, (2, 1): 2, (2, 2): 2, (3, 0): 3, (3, 1): 3, (3, 2): 3}\n"
          ]
        }
      ],
      "source": [
        "states_B   = set(range(4))\n",
        "actions_B = set(range(3))\n",
        "print(f'{states_B=} | {actions_B=}')\n",
        "T_B = {\n",
        "    (0, 0): 1,\n",
        "    (0, 1): 2,\n",
        "    (0, 2): 3,\n",
        "    \n",
        "    (1, 0): 1,\n",
        "    (1, 1): 1,\n",
        "    (1, 2): 1,\n",
        "    \n",
        "    (2, 0): 2,\n",
        "    (2, 1): 2,\n",
        "    (2, 2): 2,\n",
        "    \n",
        "    (3, 0): 3,\n",
        "    (3, 1): 3,\n",
        "    (3, 2): 3\n",
        "}\n",
        "print(f'Transition function {T_B=}')\n",
        "B_mdp = states_B, actions_B, T_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlqB4WcZ7CDK"
      },
      "source": [
        "Попробуйте записать функцию перехода в матричном виде:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jmg8hnng7EnJ"
      },
      "outputs": [],
      "source": [
        "T_matr_A = np.array([\n",
        "#    0  1  2 - итоговые состояния\n",
        "    [0, 1, 0], # 0 - исходное состояние\n",
        "    [0, 0, 1], # 1 - исходное состояние\n",
        "    [0, 0, 1]  # 2 - исходное состояние\n",
        "]) # значения элементов - вероятности перехода"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# не указаны вероятности перехода, возьмем 1/3\n",
        "T_matr_B = np.array([\n",
        "#    0  1    2    3 - итоговые состояния\n",
        "    [0, 1/3, 1/3, 1/3], # 0 - исходное состояние\n",
        "    [0, 1,   0,   0  ], # 1 - исходное состояние\n",
        "    [0, 0,   1,   0  ], # 2 - исходное состояние\n",
        "    [0, 0,   0,   1  ]  # 3 - исходное состояние\n",
        "]) # значения элементов - вероятности перехода"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FPshg_07G0R"
      },
      "source": [
        "Как получить вероятность нахождения агента в состоянии (1) через N шагов? Что происходит с вероятностями нахождения в состояниях при $N \\rightarrow \\infty$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9pwc4Atn7IAf"
      },
      "outputs": [],
      "source": [
        "def step_n(s, T_matr, n):\n",
        "    return s @ np.linalg.matrix_power(T_matr, n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MDP: A, init state: [1 0 0]\n",
            "0 steps, state: [1 0 0]\n",
            "1 steps, state: [0 1 0]\n",
            "2 steps, state: [0 0 1]\n",
            "3 steps, state: [0 0 1]\n",
            "4 steps, state: [0 0 1]\n",
            "Вероятность нахождения в состоянии 1 при старте в состоянии 0 при N = 1 равна 1, при N != 1 равна 0\n",
            "При N->inf попадаем в стационартное состояние\n"
          ]
        }
      ],
      "source": [
        "init_state_A = np.array([1, 0, 0]) # возьмем состояние 0\n",
        "print(f\"MDP: A, init state: {init_state_A}\")\n",
        "for i in range(5):\n",
        "    print(f\"{i} steps, state: {step_n(init_state_A, T_matr_A, i)}\")\n",
        "print(\"Вероятность нахождения в состоянии 1 при старте в состоянии 0 при N = 1 равна 1, при N != 1 равна 0\")\n",
        "print(\"При N->inf попадаем в стационартное состояние\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MDP: B, init state: [1 0 0 0]\n",
            "0 steps, state: [1. 0. 0. 0.]\n",
            "1 steps, state: [0.         0.33333333 0.33333333 0.33333333]\n",
            "2 steps, state: [0.         0.33333333 0.33333333 0.33333333]\n",
            "3 steps, state: [0.         0.33333333 0.33333333 0.33333333]\n",
            "4 steps, state: [0.         0.33333333 0.33333333 0.33333333]\n",
            "Вероятность нахождения в состоянии 1 при старте в состоянии 0 при N = 0 равна 0, при N > 1 равна 1/3\n",
            "При N->inf попадаем в стационартное состояние\n"
          ]
        }
      ],
      "source": [
        "init_state_B = np.array([1, 0, 0, 0]) # возьмем состояние 0\n",
        "print(f\"MDP: B, init state: {init_state_B}\")\n",
        "for i in range(5):\n",
        "    print(f\"{i} steps, state: {step_n(init_state_B, T_matr_B, i)}\")\n",
        "print(\"Вероятность нахождения в состоянии 1 при старте в состоянии 0 при N = 0 равна 0, при N > 1 равна 1/3\")\n",
        "print(\"При N->inf попадаем в стационартное состояние\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Причем, возможно, я запутался в терминологоии:\n",
        "- я записал функцию перехода в матричном виде для состояний (она мне понадобилась для расчета нахождения состояния через N шагов)\n",
        "- можно записать функцию перехода в матричном виде для среды (это будет матрица размера N_states x N_actions x N_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p52R04np7Ku0"
      },
      "source": [
        "Задайте еще несколько MDP:\n",
        "\n",
        "- C: <img src=https://i.ibb.co/Jj9LYHjP/mdp-c.png caption=\"C\" width=\"400\" height=\"100\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JSdJ9ZsI7Nfw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "states_C={0, 1, 2, 3} | actions_C={0, 1}\n",
            "Transition function T_C={(0, 0): 1, (0, 1): 2, (1, 0): 1, (1, 1): 3, (2, 0): 3, (2, 1): 2, (3, 0): 3, (3, 1): 3}\n",
            "MDP: C, init state: [1 0 0 0]\n",
            "0 steps, state: [1. 0. 0. 0.]\n",
            "1 steps, state: [0.  0.5 0.5 0. ]\n",
            "2 steps, state: [0.   0.25 0.25 0.5 ]\n",
            "3 steps, state: [0.    0.125 0.125 0.75 ]\n",
            "4 steps, state: [0.     0.0625 0.0625 0.875 ]\n",
            "Вероятность нахождения в состоянии 1 при старте в состоянии 0 при N = 0 равна 0, при N > 0 равна (1/2)^N\n",
            "При N->inf попадаем в стационартное состояние\n"
          ]
        }
      ],
      "source": [
        "states_C   = set(range(4))\n",
        "actions_C = set(range(2))\n",
        "print(f'{states_C=} | {actions_C=}')\n",
        "T_C = {\n",
        "    (0, 0): 1,\n",
        "    (0, 1): 2,\n",
        "    \n",
        "    (1, 0): 1,\n",
        "    (1, 1): 3,\n",
        "    \n",
        "    (2, 0): 3,\n",
        "    (2, 1): 2,\n",
        "    \n",
        "    (3, 0): 3,\n",
        "    (3, 1): 3\n",
        "}\n",
        "print(f'Transition function {T_C=}')\n",
        "C_mdp = states_C, actions_C, T_C\n",
        "\n",
        "# не указаны вероятности перехода, возьмем 1/2\n",
        "T_matr_C = np.array([\n",
        "#    0  1    2    3 - итоговые состояния\n",
        "    [0, 1/2, 1/2, 0  ], # 0 - исходное состояние\n",
        "    [0, 1/2, 0,   1/2], # 1 - исходное состояние\n",
        "    [0, 0,   1/2, 1/2], # 2 - исходное состояние\n",
        "    [0, 0,   0,   1  ]  # 3 - исходное состояние\n",
        "]) # значения элементов - вероятности перехода\n",
        "\n",
        "init_state_C = np.array([1, 0, 0, 0]) # возьмем состояние 0\n",
        "print(f\"MDP: C, init state: {init_state_C}\")\n",
        "for i in range(5):\n",
        "    print(f\"{i} steps, state: {step_n(init_state_C, T_matr_C, i)}\")\n",
        "print(\"Вероятность нахождения в состоянии 1 при старте в состоянии 0 при N = 0 равна 0, при N > 0 равна (1/2)^N\")\n",
        "print(\"При N->inf попадаем в стационартное состояние\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiU5X4DH7TaG"
      },
      "source": [
        "Давайте попробуем задать двух агентов: случайного и оптимального (для каждой среды свой)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "It2waXJi7WWN"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, actions):\n",
        "        self.rng = np.random.default_rng()\n",
        "        self.actions = np.array(list(actions))\n",
        "\n",
        "    def act(self, state):\n",
        "        return self.rng.integers(len(self.actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimalAgentA:\n",
        "    def __init__(self, actions):\n",
        "        self.actions = np.array(list(actions))\n",
        "\n",
        "    def act(self, state):\n",
        "        # одно действие\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Других оптимальных агентов зададим ниже, так как для этого надо понять, что такое \"оптимальный\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH6uo4EP7ZqB"
      },
      "source": [
        "В качестве дополнения, запишите стратегию агента"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Если я правильно понял, то под стратегией мы подразумеваем как раз правило, по которому агент выбирает действие  \n",
        "Это правило в свою очередь будет как раз заложено в класс оптимального агента"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGx2-KeH7lL4"
      },
      "source": [
        "## Reward, reward function\n",
        "\n",
        "Теперь добавим произвольную функцию вознаграждения. Например, для A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fk7umEnA7oFv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{(0, 0): -0.1, (1, 0): 1.0, (2, 0): 0.0}\n",
            "({0, 1, 2}, {0}, {(0, 0): 1, (1, 0): 2, (2, 0): 2}, {(0, 0): -0.1, (1, 0): 1.0, (2, 0): 0.0})\n"
          ]
        }
      ],
      "source": [
        "R_A = {\n",
        "    (0, 0): -0.1,\n",
        "    (1, 0): 1.0,\n",
        "    (2, 0): 0.0\n",
        "}\n",
        "print(R_A)\n",
        "\n",
        "A_mdp = *A_mdp, R_A\n",
        "print(A_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{(0, 0): -1, (0, 1): 0, (0, 2): 1, (1, 0): 0, (1, 1): 0, (1, 2): 0, (2, 0): 0, (2, 1): 0, (2, 2): 0, (3, 0): 0, (3, 1): 0, (3, 2): 0}\n",
            "({0, 1, 2, 3}, {0, 1, 2}, {(0, 0): 1, (0, 1): 2, (0, 2): 3, (1, 0): 1, (1, 1): 1, (1, 2): 1, (2, 0): 2, (2, 1): 2, (2, 2): 2, (3, 0): 3, (3, 1): 3, (3, 2): 3}, {(0, 0): -1, (0, 1): 0, (0, 2): 1, (1, 0): 0, (1, 1): 0, (1, 2): 0, (2, 0): 0, (2, 1): 0, (2, 2): 0, (3, 0): 0, (3, 1): 0, (3, 2): 0})\n"
          ]
        }
      ],
      "source": [
        "# Зададим награды за переходы \n",
        "R_B = {\n",
        "    (0, 0): -1,\n",
        "    (0, 1): 0,\n",
        "    (0, 2): 1,\n",
        "    \n",
        "    (1, 0): 0,\n",
        "    (1, 1): 0,\n",
        "    (1, 2): 0,\n",
        "    \n",
        "    (2, 0): 0,\n",
        "    (2, 1): 0,\n",
        "    (2, 2): 0,\n",
        "    \n",
        "    (3, 0): 0,\n",
        "    (3, 1): 0,\n",
        "    (3, 2): 0\n",
        "}\n",
        "print(R_B)\n",
        "\n",
        "B_mdp = *B_mdp, R_B\n",
        "print(B_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{(0, 0): -5, (0, 1): 10, (1, 0): 20, (1, 1): 1, (2, 0): -5, (2, 1): 10, (3, 0): 0, (3, 1): 0}\n",
            "({0, 1, 2, 3}, {0, 1}, {(0, 0): 1, (0, 1): 2, (1, 0): 1, (1, 1): 3, (2, 0): 3, (2, 1): 2, (3, 0): 3, (3, 1): 3}, {(0, 0): -5, (0, 1): 10, (1, 0): 20, (1, 1): 1, (2, 0): -5, (2, 1): 10, (3, 0): 0, (3, 1): 0})\n"
          ]
        }
      ],
      "source": [
        "# Зададим награды за переходы \n",
        "# Предполагается, что если у агента много времени, то ему будет выгодно пойти сначала на убыль 1, чтобы в цикле состоянии набрать больше\n",
        "R_C = {\n",
        "    (0, 0): -5,\n",
        "    (0, 1): 10,\n",
        "    \n",
        "    (1, 0): 20,\n",
        "    (1, 1): 1,\n",
        "    \n",
        "    (2, 0): -5,\n",
        "    (2, 1): 10,\n",
        "    \n",
        "    (3, 0): 0,\n",
        "    (3, 1): 0\n",
        "}\n",
        "print(R_C)\n",
        "\n",
        "C_mdp = *C_mdp, R_C\n",
        "print(C_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimalAgentB:\n",
        "    def __init__(self, actions):\n",
        "        self.actions = np.array(list(actions))\n",
        "\n",
        "    def act(self, state):\n",
        "        optimal_moves = {\n",
        "            0: 2,\n",
        "            1: 0,\n",
        "            2: 0,\n",
        "            3: 0,\n",
        "        }\n",
        "        return optimal_moves[state]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimalAgentC:\n",
        "    def __init__(self, actions):\n",
        "        self.actions = np.array(list(actions))\n",
        "\n",
        "    def act(self, state):\n",
        "        optimal_moves = {\n",
        "            0: 0, # с поправкой, что у агента много времени\n",
        "            1: 0,\n",
        "            2: 1,\n",
        "            3: 0,\n",
        "        }\n",
        "        return optimal_moves[state]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j92TZ1l67rVh"
      },
      "source": [
        "## Interaction loop, trajectory, termination, truncation, episode\n",
        "\n",
        "Общий цикл взаимодействия в рамках эпизода:\n",
        "1. Инициализировать среду: $s \\leftarrow \\text{env.init()}$\n",
        "2. Цикл:\n",
        "    - выбрать действие: $a \\leftarrow \\pi(s)$\n",
        "    - получить ответ от среды: $s, r, d \\leftarrow \\text{env.next(a)}$\n",
        "    - если $d == \\text{True}$, выйти из цикла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MRPZACJt7vG8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, np.int64(0), -0.1),\n",
              " (1, np.int64(0), 1.0),\n",
              " (2, np.int64(0), 0.0),\n",
              " (2, np.int64(0), 0.0),\n",
              " (2, np.int64(0), 0.0)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def run_episode(mdp):\n",
        "    states, actions, T, R = mdp\n",
        "    agent = Agent(actions)\n",
        "\n",
        "    s = 0\n",
        "    tau = []\n",
        "    for _ in range(5):\n",
        "        a = agent.act(s)\n",
        "        s_next = T[(s, a)]\n",
        "        r = R[(s, a)]\n",
        "\n",
        "        tau.append((s, a, r))\n",
        "        s = s_next\n",
        "\n",
        "    return tau\n",
        "\n",
        "run_episode(A_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, np.int64(2), 1),\n",
              " (3, np.int64(2), 0),\n",
              " (3, np.int64(0), 0),\n",
              " (3, np.int64(1), 0),\n",
              " (3, np.int64(0), 0)]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_episode(B_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, np.int64(1), 10),\n",
              " (2, np.int64(1), 10),\n",
              " (2, np.int64(1), 10),\n",
              " (2, np.int64(0), -5),\n",
              " (3, np.int64(1), 0)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_episode(C_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 0, -0.1), (1, 0, 1.0), (2, 0, 0.0), (2, 0, 0.0), (2, 0, 0.0)]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def run_episode_with_custom_agent(mdp, agent):\n",
        "    states, actions, T, R = mdp\n",
        "    ag = agent(actions)\n",
        "\n",
        "    s = 0\n",
        "    tau = []\n",
        "    for _ in range(5):\n",
        "        a = ag.act(s)\n",
        "        s_next = T[(s, a)]\n",
        "        r = R[(s, a)]\n",
        "\n",
        "        tau.append((s, a, r))\n",
        "        s = s_next\n",
        "\n",
        "    return tau\n",
        "\n",
        "run_episode_with_custom_agent(A_mdp, OptimalAgentA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 2, 1), (3, 0, 0), (3, 0, 0), (3, 0, 0), (3, 0, 0)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_episode_with_custom_agent(B_mdp, OptimalAgentB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 0, -5), (1, 0, 20), (1, 0, 20), (1, 0, 20), (1, 0, 20)]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_episode_with_custom_agent(C_mdp, OptimalAgentC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbGEr8kl7xnS"
      },
      "source": [
        "Termination — означает окончание эпизода, когда достигнуто терминальное состояние. Является частью задания среды.\n",
        "\n",
        "Truncation — означает окончание эпизода, когда достигнут лимит по числу шагов (=времени). Обычно является внешне заданным параметром для удобства обучения.\n",
        "\n",
        "Пока не будем вводить truncation, но поддержим termination: расширьте определение среды информацией о терминальных состояниях для всех описанных ранее сред. Сгенерируйте по несколько случайных траекторий для каждой среды."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "A_terms = set([2])\n",
        "A_mdp = *A_mdp, A_terms\n",
        "\n",
        "B_terms = set([1, 2, 3])\n",
        "B_mdp = *B_mdp, B_terms\n",
        "\n",
        "C_terms = set([3])\n",
        "C_mdp = *C_mdp, C_terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_random_trajectory(mdp, truncation=10):\n",
        "    states, actions, T, R, terms = mdp\n",
        "    state = np.random.choice(list(states))\n",
        "    trajectory = []\n",
        "    for step in range(truncation):\n",
        "        terminated = state in terms\n",
        "        if terminated:\n",
        "            trajectory.append({\n",
        "                'step': step,\n",
        "                'state': state,\n",
        "                'terminated': terminated\n",
        "            })\n",
        "            break\n",
        "        action = np.random.choice(list(actions))\n",
        "        next_state = T[(state, action)] \n",
        "        reward = R[(state, action)]\n",
        "        trajectory.append({\n",
        "            'step': step,\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'terminated': terminated\n",
        "        })\n",
        "        state = next_state\n",
        "    return trajectory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'step': 0,\n",
              "  'state': np.int64(0),\n",
              "  'action': np.int64(0),\n",
              "  'reward': -0.1,\n",
              "  'next_state': 1,\n",
              "  'terminated': False},\n",
              " {'step': 1,\n",
              "  'state': 1,\n",
              "  'action': np.int64(0),\n",
              "  'reward': 1.0,\n",
              "  'next_state': 2,\n",
              "  'terminated': False},\n",
              " {'step': 2, 'state': 2, 'terminated': True}]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a_tr = gen_random_trajectory(A_mdp)\n",
        "a_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'step': 0, 'state': np.int64(1), 'terminated': True}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b_tr = gen_random_trajectory(B_mdp)\n",
        "b_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'step': 0,\n",
              "  'state': np.int64(2),\n",
              "  'action': np.int64(1),\n",
              "  'reward': 10,\n",
              "  'next_state': 2,\n",
              "  'terminated': False},\n",
              " {'step': 1,\n",
              "  'state': 2,\n",
              "  'action': np.int64(1),\n",
              "  'reward': 10,\n",
              "  'next_state': 2,\n",
              "  'terminated': False},\n",
              " {'step': 2,\n",
              "  'state': 2,\n",
              "  'action': np.int64(1),\n",
              "  'reward': 10,\n",
              "  'next_state': 2,\n",
              "  'terminated': False},\n",
              " {'step': 3,\n",
              "  'state': 2,\n",
              "  'action': np.int64(0),\n",
              "  'reward': -5,\n",
              "  'next_state': 3,\n",
              "  'terminated': False},\n",
              " {'step': 4, 'state': 3, 'terminated': True}]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c_tr = gen_random_trajectory(C_mdp)\n",
        "c_tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxHNM9kS74WW"
      },
      "source": [
        "### Return, expected return\n",
        "\n",
        "Наиболее важная метрика оценки качества работы агента: отдача.\n",
        "\n",
        "Отдача: $G(s_t) = \\sum_{i=t}^T r_i$\n",
        "\n",
        "Обычно также вводят параметр $\\gamma \\in [0, 1]$, дисконтирующий будущие вознаграждения. А еще, тк отдача может меняться от запуска к запуску благодаря вероятностным процессам, нас интересует отдача в среднем — ожидаемая отдача:\n",
        "\n",
        "$$\\hat{G}(s_t) = \\mathbb{E} [ \\sum_{i=t}^T \\gamma^{i-t} r_i ]$$\n",
        "\n",
        "Именно ее и оптимизируют в RL.\n",
        "\n",
        "Давайте научимся считать отдачу для состояний по траектории и считать среднюю отдачу."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "scdThsNA8T2B"
      },
      "outputs": [],
      "source": [
        "def calculate_returns(trajectory, gamma=0.9):\n",
        "    returns = {}\n",
        "\n",
        "    for t, experience in enumerate(trajectory):\n",
        "        state = experience['state']\n",
        "        \n",
        "        if 'reward' not in experience:\n",
        "            continue\n",
        "            \n",
        "        G = 0\n",
        "        for i in range(t, len(trajectory)):\n",
        "            if 'reward' in trajectory[i]:\n",
        "                reward = trajectory[i]['reward']\n",
        "                G += (gamma ** (i - t)) * reward\n",
        "        \n",
        "        returns[state] = G\n",
        "\n",
        "    return returns\n",
        "\n",
        "def calculate_average_returns(mdp, num_episodes=1000, gamma=0.9, truncation=100):\n",
        "    state_returns = {}\n",
        "    \n",
        "    # Генерируем много траекторий\n",
        "    for _ in range(num_episodes):\n",
        "        trajectory = gen_random_trajectory(mdp, truncation)\n",
        "        \n",
        "        # Вычисляем отдачу для этой траектории\n",
        "        episode_returns = calculate_returns(trajectory, gamma)\n",
        "        \n",
        "        # Добавляем к общей статистике\n",
        "        for state, return_value in episode_returns.items():\n",
        "            if state not in state_returns:\n",
        "                state_returns[state] = []\n",
        "            state_returns[state].append(return_value)\n",
        "    \n",
        "    # Вычисляем среднюю отдачу для каждого состояния\n",
        "    average_returns = {}\n",
        "    for state, returns_list in state_returns.items():\n",
        "        average_returns[state] = sum(returns_list) / len(returns_list)\n",
        "    \n",
        "    return average_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{np.int64(0): 0.8, 1: 1.0}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_returns(a_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_returns(b_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{np.int64(2): -5.0}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_returns(c_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{np.int64(0): 0.8, 1: 1.0}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_average_returns(A_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{np.int64(0): -0.06854838709677419}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_average_returns(B_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{np.int64(1): 1.0, np.int64(0): 12.198287316963654, 2: -5.0}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_average_returns(C_mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
